#!/usr/bin/env python3
"""
LMS Content Pipeline - Gemini Edition
=======================================
A complete prompt-chaining pipeline for generating Canvas LMS-ready educational content
with optional AI-generated illustrations, powered by Google Gemini.

Pipeline Stages:
  Agent A: Content Writer    ‚Üí Drafts educational content
  Agent B: HTML Structurer   ‚Üí Converts to semantic HTML
  Agent C: Publisher/Styler  ‚Üí Applies accessibility styles and Knowledge Checks
  Agent D: Illustrator       ‚Üí Generates, uploads (ImageKit), and embeds AI images

Supported Content Types:
  - Textbook: Full chapters with Knowledge Checks and illustrations
  - Discussion: Interactive discussion prompts with Font Awesome icons
  - Assignment: Step-based assignments with rubrics

Output Structure:
  lms_output/
  ‚îî‚îÄ‚îÄ YYYY-MM-DD_HH-MM-SS_<content-type>/
      ‚îú‚îÄ‚îÄ content/
      ‚îÇ   ‚îú‚îÄ‚îÄ Step1_Content.txt
      ‚îÇ   ‚îú‚îÄ‚îÄ Step2_Structured.html
      ‚îÇ   ‚îú‚îÄ‚îÄ Step3_Styled.html
      ‚îÇ   ‚îî‚îÄ‚îÄ Step4_Final.html (with images, if enabled)
      ‚îú‚îÄ‚îÄ images/
      ‚îÇ   ‚îú‚îÄ‚îÄ fig1-YYYY-MM-DD_HH-MM-SS.png
      ‚îÇ   ‚îú‚îÄ‚îÄ fig2-YYYY-MM-DD_HH-MM-SS.png
      ‚îÇ   ‚îî‚îÄ‚îÄ fig3-YYYY-MM-DD_HH-MM-SS.png
      ‚îî‚îÄ‚îÄ pipeline.log

Usage:
  python gemini_lms_pipeline.py                    # Interactive mode
  python gemini_lms_pipeline.py --no-images        # Skip image generation
  python gemini_lms_pipeline.py --topic "AI"       # Provide topic directly

Requires Environment Variables:
  - GOOGLE_API_KEY or GEMINI_API_KEY (required)
  - IMAGEKIT_PRIVATE_KEY, IMAGEKIT_PUBLIC_KEY, IMAGEKIT_URL_ENDPOINT (required for images)

Dependencies:
  pip install google-genai imagekitio

Author: Refactored for Gemini
"""

from __future__ import annotations

import argparse
import html
import json
import os
import re
import sys
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Final, List, Optional, Dict, Any

try:
    from google import genai
    from google.genai import types
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("Error: google-genai library not found. Please install it via 'pip install google-genai'")

try:
    from imagekitio import ImageKit
    IMAGEKIT_AVAILABLE = True
except ImportError:
    IMAGEKIT_AVAILABLE = False
    print("Warning: imagekitio library not found. Images will be local only if not installed.")


# =============================================================================
# CONFIGURATION
# =============================================================================

class ContentType(Enum):
    """Supported LMS content types."""
    TEXTBOOK = "textbook"
    DISCUSSION = "discussion"
    ASSIGNMENT = "assignment"


# Model Configuration
GEMINI_TEXT_MODEL: Final[str] = "gemini-3-flash-preview"
GEMINI_IMAGE_MODEL: Final[str] = "gemini-3-pro-image-preview"

# Limits
MAX_IMAGES: Final[int] = 3

# Paths
PROMPTS_DIR = Path("prompts")
DEFAULT_IMAGEKIT_FOLDER: Final[str] = "/lms-content/"

# =============================================================================
# DATA STRUCTURES
# =============================================================================

@dataclass
class ImagePlan:
    """Represents a planned image with its metadata."""
    insertion_context: str
    image_prompt: str
    alt_text: str
    caption: str
    figure_number: int
    local_path: Optional[Path] = None
    hosted_url: Optional[str] = None


@dataclass
class PipelineConfig:
    """Configuration for a pipeline run."""
    content_type: ContentType
    topic: str
    timestamp: str
    output_dir: Path
    content_dir: Path
    images_dir: Path
    enable_images: bool
    imagekit_folder: str = DEFAULT_IMAGEKIT_FOLDER
    agent_a_instruction: Optional[str] = None


@dataclass
class PipelineState:
    """Holds state during pipeline execution."""
    config: PipelineConfig
    gemini_client: Optional[object] = None
    imagekit_client: Optional[object] = None
    step_outputs: Dict[str, str] = field(default_factory=dict)
    image_plans: List[ImagePlan] = field(default_factory=list)
    log_messages: List[str] = field(default_factory=list)

    def log(self, message: str, print_it: bool = True) -> None:
        """Log a message and optionally print it."""
        timestamped = f"[{datetime.now().strftime('%H:%M:%S')}] {message}"
        self.log_messages.append(timestamped)
        if print_it:
            print(message)

    def save_log(self) -> None:
        """Save the log to a file."""
        log_path = self.config.output_dir / "pipeline.log"
        log_path.write_text("\n".join(self.log_messages), encoding="utf-8")


# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def validate_environment(need_images: bool) -> tuple[bool, list[str]]:
    """Check required environment variables."""
    missing = []
    
    if not (os.environ.get("GOOGLE_API_KEY") or os.environ.get("GEMINI_API_KEY")):
        missing.append("GOOGLE_API_KEY / GEMINI_API_KEY")
        
    if need_images:
        if not os.environ.get("IMAGEKIT_PRIVATE_KEY"):
            missing.append("IMAGEKIT_PRIVATE_KEY")
        if not os.environ.get("IMAGEKIT_PUBLIC_KEY"):
            missing.append("IMAGEKIT_PUBLIC_KEY")
        if not os.environ.get("IMAGEKIT_URL_ENDPOINT"):
            missing.append("IMAGEKIT_URL_ENDPOINT")
            
    return len(missing) == 0, missing


def load_prompt(filename: str) -> str:
    """Load a prompt from the prompts directory."""
    path = PROMPTS_DIR / filename
    if not path.exists():
        raise FileNotFoundError(f"Prompt file not found: {path}")
    return path.read_text(encoding="utf-8")


def clean_json_response(text: str) -> str:
    """Remove markdown code fences and fix common JSON issues."""
    text = text.strip()
    if text.startswith("```"):
        text = re.sub(r"^```\w*\s*", "", text)
        text = re.sub(r"\s*```$", "", text)
    
    # Try to extract JSON if wrapped in other text
    first_brace = text.find('{')
    last_brace = text.rfind('}')
    
    if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
        text = text[first_brace:last_brace + 1]
        
    return text.strip()


def escape_control_chars_in_strings(text: str) -> str:
    """Escape control characters inside JSON string values."""
    result = []
    in_string = False
    escape = False

    for ch in text:
        if escape:
            result.append(ch)
            escape = False
            continue
        if ch == "\\":
            result.append(ch)
            escape = True
            continue
        if ch == "\"":
            in_string = not in_string
            result.append(ch)
            continue
        if in_string:
            if ch == "\n":
                result.append("\\n")
                continue
            if ch == "\r":
                result.append("\\r")
                continue
            if ch == "\t":
                result.append("\\t")
                continue
        result.append(ch)

    return "".join(result)


def parse_json_response(text: str) -> tuple[Optional[dict], Optional[str]]:
    """Parse JSON with light repair for common model output issues."""
    cleaned = clean_json_response(text)
    try:
        return json.loads(cleaned), None
    except json.JSONDecodeError as json_err:
        repaired = escape_control_chars_in_strings(cleaned)
        repaired = remove_trailing_commas_outside_strings(repaired)
        try:
            return json.loads(repaired), None
        except json.JSONDecodeError as json_err_repaired:
            return None, str(json_err_repaired)


def save_debug_response(state: PipelineState, stage: str, attempt: int, response_text: str) -> Optional[Path]:
    """Persist raw model output to help debug parse failures."""
    safe_stage = re.sub(r"[^a-z0-9_]+", "_", stage.lower()).strip("_")
    filename = f"debug_{safe_stage}_attempt{attempt}.txt"
    debug_path = state.config.output_dir / filename
    try:
        debug_path.write_text(response_text, encoding="utf-8")
        return debug_path
    except OSError:
        return None


def remove_trailing_commas_outside_strings(text: str) -> str:
    """Remove trailing commas that appear outside of JSON string values."""
    result = []
    in_string = False
    escape = False
    length = len(text)

    i = 0
    while i < length:
        ch = text[i]
        if escape:
            result.append(ch)
            escape = False
            i += 1
            continue
        if ch == "\\":
            result.append(ch)
            escape = True
            i += 1
            continue
        if ch == "\"":
            in_string = not in_string
            result.append(ch)
            i += 1
            continue
        if not in_string and ch == ",":
            j = i + 1
            while j < length and text[j].isspace():
                j += 1
            if j < length and text[j] in "}]":
                i += 1
                continue
        result.append(ch)
        i += 1

    return "".join(result)


def word_to_pattern(word: str) -> str:
    """Convert a word to a regex pattern allowing HTML entities."""
    result = []
    for char in word:
        if char in '‚Äî‚Äì':
            result.append(r'(?:&mdash;|&ndash;|&#8212;|&#8211;|‚Äî|‚Äì|-)')
        elif char == '"':
            result.append(r'(?:&rdquo;|&#8221;|"|")')
        elif char == '"':
            result.append(r'(?:&ldquo;|&#8220;|"|")')
        elif char == "'":
            result.append(r"(?:&lsquo;|&#8216;|'|')")
        elif char == "'":
            result.append(r"(?:&rsquo;|&#8217;|'|')")
        elif char == '‚Ä¶':
            result.append(r'(?:&hellip;|&#8230;|‚Ä¶|‚Ä¶)')
        elif char == '&':
            result.append(r'(?:&amp;|&)')
        elif char == '<':
            result.append(r'(?:&lt;|<)')
        elif char == '>':
            result.append(r'(?:&gt;|>)')
        elif char == ' ':
            result.append(r'\s+')
        else:
            result.append(re.escape(char))
    return ''.join(result)


# =============================================================================
# PIPELINE STAGES
# =============================================================================

def stage_a_write_content(state: PipelineState) -> bool:
    """Agent A: Write educational content."""
    state.log("\nüìù [1/4] Agent A: Writing content...")

    if state.config.agent_a_instruction:
        system_prompt = state.config.agent_a_instruction
    else:
        prompt_file = f"agent_a_{state.config.content_type.value}.txt"
        try:
            system_prompt = load_prompt(prompt_file)
        except FileNotFoundError as e:
            state.log(f"    ‚ùå Error: {e}")
            return False

    try:
        response = state.gemini_client.models.generate_content(
            model=GEMINI_TEXT_MODEL,
            contents=state.config.topic,
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                temperature=1.0,
                max_output_tokens=12000,
            ),
        )

        response_text = response.text

        if not response_text.strip():
            state.log("    ‚ùå Empty response from Agent A")
            return False

        # Save output
        output_path = state.config.content_dir / "Step1_Content.txt"
        output_path.write_text(response_text, encoding="utf-8")
        state.step_outputs["agent_a"] = response_text

        word_count = len(response_text.split())
        state.log(f"    ‚úì Generated {word_count:,} words ‚Üí {output_path.name}")
        return True

    except Exception as e:
        state.log(f"    ‚ùå Agent A Error: {e}")
        return False


def stage_b_structure_html(state: PipelineState) -> bool:
    """Agent B: Convert to semantic HTML."""
    state.log("\nüèóÔ∏è  [2/4] Agent B: Structuring HTML...")

    prompt_file = f"agent_b_{state.config.content_type.value}.txt"
    try:
        system_prompt = load_prompt(prompt_file)
    except FileNotFoundError as e:
        state.log(f"    ‚ùå Error: {e}")
        return False

    try:
        response = state.gemini_client.models.generate_content(
            model=GEMINI_TEXT_MODEL,
            contents=state.step_outputs["agent_a"],
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                temperature=0.2,
                max_output_tokens=14000,
            ),
        )

        response_text = response.text

        if not response_text.strip():
            state.log("    ‚ùå Empty response from Agent B")
            return False

        output_path = state.config.content_dir / "Step2_Structured.html"
        output_path.write_text(response_text, encoding="utf-8")
        state.step_outputs["agent_b"] = response_text

        state.log(f"    ‚úì Structured HTML ‚Üí {output_path.name}")
        return True

    except Exception as e:
        state.log(f"    ‚ùå Agent B Error: {e}")
        return False


def stage_c_style_publish(state: PipelineState) -> bool:
    """Agent C: Apply styles and accessibility features."""
    state.log("\nüé® [3/4] Agent C: Styling and publishing...")

    if state.config.content_type == ContentType.TEXTBOOK:
        prompt_file = "agent_c_textbook.txt"
    else:
        prompt_file = "agent_c_discussion_assignment.txt"

    try:
        system_prompt = load_prompt(prompt_file)
    except FileNotFoundError as e:
        state.log(f"    ‚ùå Error: {e}")
        return False

    try:
        response = state.gemini_client.models.generate_content(
            model=GEMINI_TEXT_MODEL,
            contents=state.step_outputs["agent_b"],
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                temperature=0.2,
                max_output_tokens=16384,
            ),
        )

        response_text = response.text

        if not response_text.strip():
            state.log("    ‚ùå Empty response from Agent C")
            return False

        output_path = state.config.content_dir / "Step3_Styled.html"
        output_path.write_text(response_text, encoding="utf-8")
        state.step_outputs["agent_c"] = response_text

        char_count = len(response_text)
        state.log(f"    ‚úì Styled HTML ({char_count:,} chars) ‚Üí {output_path.name}")
        return True

    except Exception as e:
        state.log(f"    ‚ùå Agent C Error: {e}")
        return False


def stage_d_brainstorm_images(state: PipelineState) -> bool:
    """Agent D Part 1: Brainstorm image insertion points."""
    state.log("\nüß† [4a/4] Agent D: Brainstorming images...")

    try:
        system_prompt = load_prompt("agent_d_brainstorm.txt")
    except FileNotFoundError as e:
        state.log(f"    ‚ùå Error: {e}")
        return False

    try:
        response = state.gemini_client.models.generate_content(
            model=GEMINI_TEXT_MODEL,
            contents=f"Analyze this HTML content and identify up to {MAX_IMAGES} optimal locations for educational images:\n\n{state.step_outputs['agent_c']}",
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                temperature=0.3,
                max_output_tokens=8192,
                response_mime_type="application/json"
            ),
        )

        response_text = response.text
        data, parse_error = parse_json_response(response_text)
        if data is None:
            state.log(f"    ‚ùå Failed to parse JSON: {parse_error}")
            state.log(f"    Debug - Raw Response: {response_text[:200]}...")
            debug_path = save_debug_response(state, "agent_d_brainstorm", 1, response_text)
            if debug_path:
                state.log(f"    Debug - Raw response saved to: {debug_path.name}")
            state.log("    Retrying with stricter JSON-only instructions...")
            retry_prompt = (
                "Return ONLY valid minified JSON. Do not include literal newlines inside string values; "
                "use \\n instead. If unsure, return {\"images\": []}.\n\n"
                f"Analyze this HTML content and identify up to {MAX_IMAGES} optimal locations for educational images:\n\n"
                f"{state.step_outputs['agent_c']}"
            )
            response = state.gemini_client.models.generate_content(
                model=GEMINI_TEXT_MODEL,
                contents=retry_prompt,
                config=types.GenerateContentConfig(
                    system_instruction=system_prompt,
                    temperature=0.2,
                    max_output_tokens=8192,
                    response_mime_type="application/json"
                ),
            )
            response_text = response.text
            data, parse_error = parse_json_response(response_text)
            if data is None:
                state.log(f"    ‚ùå Failed to parse JSON after retry: {parse_error}")
                state.log(f"    Debug - Raw Response: {response_text[:200]}...")
                debug_path = save_debug_response(state, "agent_d_brainstorm", 2, response_text)
                if debug_path:
                    state.log(f"    Debug - Raw response saved to: {debug_path.name}")
                return False

        if isinstance(data, dict):
            raw_images = data.get("images", [])
        elif isinstance(data, list):
            raw_images = data
        else:
            raw_images = []

        if not isinstance(raw_images, list):
            state.log("    ‚ö†Ô∏è Agent D response schema unexpected; no images parsed.")
            raw_images = []

        skipped = 0
        for i, item in enumerate(raw_images[:MAX_IMAGES], start=1):
            if not isinstance(item, dict):
                skipped += 1
                continue
            insertion_context = str(item.get("insertion_context", "")).strip()
            image_prompt = str(item.get("image_prompt", "")).strip()
            alt_text = str(item.get("alt_text", "")).strip()
            caption = str(item.get("caption", "")).strip() or f"Figure {i}: Illustration"

            if not insertion_context or not image_prompt or not alt_text:
                skipped += 1
                continue

            plan = ImagePlan(
                insertion_context=insertion_context,
                image_prompt=image_prompt,
                alt_text=alt_text[:125],
                caption=caption,
                figure_number=i,
            )
            state.image_plans.append(plan)

        if skipped:
            state.log(f"    ‚ö†Ô∏è Skipped {skipped} malformed image entries")
        state.log(f"    ‚úì Identified {len(state.image_plans)} image insertion points")
        return True

    except Exception as e:
        state.log(f"    ‚ùå Agent D Error: {e}")
        return False


def stage_d_generate_images(state: PipelineState) -> bool:
    """Agent D Part 2: Generate images, save locally, and upload to ImageKit."""
    state.log(f"\nüñºÔ∏è  [4b/4] Generating {len(state.image_plans)} images...")

    successful_plans = []

    for plan in state.image_plans:
        state.log(f"    Figure {plan.figure_number}: Generating...")

        try:
            full_prompt = (
                f"Generate a high-quality educational illustration: {plan.image_prompt}. "
                f"Style: Professional, clean, minimal text overlay, suitable for a textbook."
            )

            response = state.gemini_client.models.generate_content(
                model=GEMINI_IMAGE_MODEL,
                contents=full_prompt,
                config=types.GenerateContentConfig(response_modalities=["IMAGE"]),
            )

            image_data = None
            if response.candidates and response.candidates[0].content.parts:
                for part in response.candidates[0].content.parts:
                    if part.inline_data and part.inline_data.data:
                        image_data = part.inline_data.data
                        break
            
            if not image_data:
                state.log(f"        ‚ùå No image data received")
                continue

            # Save locally
            local_filename = f"fig{plan.figure_number}-{state.config.timestamp}.png"
            local_path = state.config.images_dir / local_filename
            with open(local_path, "wb") as f:
                f.write(image_data)
            
            plan.local_path = local_path
            state.log(f"        ‚úì Saved local: {local_filename}")

            # Upload to ImageKit
            if state.imagekit_client:
                state.log(f"        ‚Üë Uploading to ImageKit...")
                result = state.imagekit_client.files.upload(
                    file=image_data,
                    file_name=local_filename,
                    folder=state.config.imagekit_folder,
                    use_unique_file_name=True,
                    is_private_file=False,
                    tags=["lms", "educational", "ai-generated"],
                )

                if result and hasattr(result, 'url') and result.url:
                    plan.hosted_url = result.url
                    state.log(f"        ‚úì Hosted: {result.url}")
                else:
                    state.log(f"        ‚ö†Ô∏è Upload failed or no URL returned, using local path")
            
            successful_plans.append(plan)

        except Exception as e:
            state.log(f"        ‚ùå Failed: {e}")
            continue

    state.image_plans = successful_plans
    return len(successful_plans) > 0


def stage_d_inject_images(state: PipelineState) -> bool:
    """Agent D Part 3: Inject images into HTML."""
    state.log(f"\nüìé [4c/4] Injecting images into HTML...")

    html_content = state.step_outputs["agent_c"]

    for plan in state.image_plans:
        # Determine Source URL
        if plan.hosted_url:
            img_src = plan.hosted_url
        elif plan.local_path:
            # Fallback to relative path if upload failed
            img_src = f"../images/{plan.local_path.name}"
        else:
            continue

        figure_html = f"""
</p>
<figure style="margin: 2em 0; max-width: 75ch;">
  <img style="width: 100%; height: auto; border: 1px solid #E0E0E0; border-radius: 4px;"
       src="{img_src}"
       alt="{plan.alt_text}"
       loading="lazy" />
  <figcaption style="padding: 0.75em 1em; font-style: italic; background-color: #F5F5F5; text-align: center; font-family: system-ui, -apple-system, 'Segoe UI', sans-serif; font-size: 16px; color: #333333; border-bottom-left-radius: 4px; border-bottom-right-radius: 4px;">
    {plan.caption} <em>(Generated by AI)</em>
  </figcaption>
</figure>
<p style="font-family: system-ui, -apple-system, 'Segoe UI', sans-serif; font-size: 18px; line-height: 1.8; letter-spacing: 0.02em; word-spacing: 0.05em; margin-bottom: 1.5em; max-width: 75ch; color: #000000;">
"""

        context = plan.insertion_context.strip()
        normalized_context = html.unescape(context)
        # Split into tokens (words and punctuation) to handle tags inside/between punctuation
        # e.g. "word</strong>." -> "word", "."
        tokens = re.findall(r"\w+|[^\w\s]", normalized_context)

        if not tokens:
            continue

        word_patterns = [word_to_pattern(t) for t in tokens]
        separator = r'[\s\n]*(?:<[^>]+>)*[\s\n]*'
        pattern = r'(?si)' + separator.join(word_patterns)

        match = re.search(pattern, html_content)

        if not match and len(tokens) > 4:
            # Fallback: Try matching just the last 50% of tokens
            half_tokens = tokens[len(tokens)//2:]
            state.log(f"    ‚ö†Ô∏è Exact match failed. Retrying with partial context ({len(half_tokens)} tokens)...")
            
            word_patterns_fallback = [word_to_pattern(t) for t in half_tokens]
            pattern_fallback = r'(?si)' + separator.join(word_patterns_fallback)
            match = re.search(pattern_fallback, html_content)

        if match:
            end_pos = match.end()
            html_content = html_content[:end_pos] + figure_html + html_content[end_pos:]
            state.log(f"    ‚úì Inserted Figure {plan.figure_number}")
        else:
            state.log(f"    ‚ùå Insertion context not found for Figure {plan.figure_number}")
            state.log(f"       Context looked for: '{context}'")

    state.step_outputs["agent_d"] = html_content
    return True


def save_final_output(state: PipelineState) -> Path:
    """Save the final HTML output."""
    if "agent_d" in state.step_outputs:
        final_html = state.step_outputs["agent_d"]
        step_name = "Step4_Final.html"
    else:
        final_html = state.step_outputs["agent_c"]
        step_name = "Step3_Final.html"

    output_path = state.config.content_dir / step_name
    output_path.write_text(final_html, encoding="utf-8")
    
    return output_path


# =============================================================================
# USER INTERFACE
# =============================================================================

def display_banner():
    print("\n" + "=" * 65)
    print("  ‚ú® LMS CONTENT PIPELINE - Gemini Edition ‚ú®")
    print("=" * 65)


def display_menu() -> ContentType:
    print("\nWhat type of LMS content would you like to generate?\n")
    print("  [1] üìö Textbook Chapter")
    print("      Full chapter with Knowledge Checks + AI illustrations")
    print()
    print("  [2] üí¨ Discussion Prompt")
    print("      Write/Respond/Evaluate sections with icons")
    print()
    print("  [3] üìã Assignment Instructions")
    print("      Overview/Instructions/Deliverables/Rubric with icons")
    print()

    while True:
        choice = input("Enter your choice (1-3): ").strip()
        if choice == "1":
            return ContentType.TEXTBOOK
        elif choice == "2":
            return ContentType.DISCUSSION
        elif choice == "3":
            return ContentType.ASSIGNMENT
        print("Invalid choice. Please enter 1, 2, or 3.")


def get_topic_prompt(content_type: ContentType) -> str:
    prompts = {
        ContentType.TEXTBOOK: "Enter the topic for your textbook chapter",
        ContentType.DISCUSSION: "Enter the topic/scenario for your discussion",
        ContentType.ASSIGNMENT: "Enter the topic/task for your assignment",
    }
    return prompts[content_type]


def prompt_agent_a_override(content_type: ContentType) -> Optional[str]:
    print("\nWould you like to override Agent A's default prompt?")
    choice = input("Enter y/N: ").strip().lower()
    if choice not in {"y", "yes"}:
        return None

    print("\nEnter custom instructions for Agent A.")
    print("Press Enter on an empty line to finish.")
    lines = []
    while True:
        line = input()
        if not line.strip():
            break
        lines.append(line)

    instruction = "\n".join(lines).strip()
    if not instruction:
        print("No custom instructions provided. Using default prompt.")
        return None

    print(f"‚úÖ Using custom Agent A instructions for {content_type.value}.")
    return instruction


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="LMS Content Pipeline (Gemini Edition)")
    parser.add_argument("--topic", "-t", help="Topic for content generation")
    parser.add_argument("--type", "-c", choices=["textbook", "discussion", "assignment"], help="Content type")
    parser.add_argument("--agent-a-instruction", help="Override Agent A system prompt with custom instructions")
    parser.add_argument("--no-images", action="store_true", help="Skip image generation")
    parser.add_argument("--force-images", action="store_true", help="Force image generation for non-textbooks")
    return parser.parse_args()


def configure_output_encoding() -> None:
    """Best-effort UTF-8 output to avoid console encoding crashes."""
    try:
        if hasattr(sys.stdout, "reconfigure"):
            sys.stdout.reconfigure(encoding="utf-8", errors="replace")
        if hasattr(sys.stderr, "reconfigure"):
            sys.stderr.reconfigure(encoding="utf-8", errors="replace")
    except Exception:
        pass


# =============================================================================
# MAIN
# =============================================================================

def main() -> int:
    configure_output_encoding()

    if not GEMINI_AVAILABLE:
        return 1

    args = parse_args()
    display_banner()

    # Content Type
    if args.type:
        content_type = ContentType(args.type)
        print(f"\nüìã Content type: {content_type.value.capitalize()}")
    else:
        content_type = display_menu()

    agent_a_instruction = args.agent_a_instruction
    if agent_a_instruction is None and args.type is None:
        agent_a_instruction = prompt_agent_a_override(content_type)

    # Image Logic
    enable_images = (
        not args.no_images
        and (content_type == ContentType.TEXTBOOK or args.force_images)
    )

    # Validate Environment
    valid, missing = validate_environment(enable_images)
    if not valid:
        print(f"\n‚ùå Missing environment variables:")
        for var in missing:
            print(f"   ‚Ä¢ {var}")
        return 1

    if args.no_images:
        print("\nüì∑ Image generation: Disabled (--no-images)")
    elif enable_images:
        if IMAGEKIT_AVAILABLE:
            print("\nüì∑ Image generation: Enabled (ImageKit Hosting)")
        else:
            print("\nüì∑ Image generation: Enabled (Local Only - ImageKit missing)")
    else:
        print("\nüì∑ Image generation: Skipped (default for this type)")

    # Topic
    if args.topic:
        topic = args.topic
        print(f"\nüìù Topic: {topic}")
    else:
        prompt = get_topic_prompt(content_type)
        topic = input(f"\n{prompt}: ").strip()
    
    if not topic:
        print("‚ùå Topic cannot be empty.")
        return 1

    # Setup Directories
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    output_base = Path.cwd() / "lms_output"
    output_base.mkdir(exist_ok=True)
    output_dir = output_base / f"{timestamp}_{content_type.value}"
    content_dir = output_dir / "content"
    images_dir = output_dir / "images"

    output_dir.mkdir(parents=True, exist_ok=True)
    content_dir.mkdir(exist_ok=True)
    if enable_images:
        images_dir.mkdir(exist_ok=True)

    # Initialize Clients
    print("\n‚öôÔ∏è  Initializing clients...")
    gemini_client = genai.Client()
    
    imagekit_client = None
    if enable_images and IMAGEKIT_AVAILABLE:
        try:
            # ImageKit 5.x+ initialization
            imagekit_client = ImageKit(
                private_key=os.environ.get("IMAGEKIT_PRIVATE_KEY"),
                # base_url=os.environ.get("IMAGEKIT_URL_ENDPOINT") # Removed to use default API endpoint
            )
            print("   ‚úì ImageKit client ready")
        except Exception as e:
            print(f"   ‚ö†Ô∏è ImageKit init failed: {e}")

    config = PipelineConfig(
        content_type=content_type,
        topic=topic,
        timestamp=timestamp,
        output_dir=output_dir,
        content_dir=content_dir,
        images_dir=images_dir,
        enable_images=enable_images,
        imagekit_folder=os.environ.get("IMAGEKIT_FOLDER", DEFAULT_IMAGEKIT_FOLDER),
        agent_a_instruction=agent_a_instruction,
    )

    state = PipelineState(
        config=config,
        gemini_client=gemini_client,
        imagekit_client=imagekit_client
    )

    prompt_file = f"agent_a_{content_type.value}.txt"
    state.log(f"Content type: {content_type.value}")
    state.log(f"Topic: {topic}")
    if agent_a_instruction:
        state.log("Agent A prompt override: enabled")
        state.log(f"Agent A custom instructions:\n{agent_a_instruction}")
    else:
        state.log(f"Agent A prompt file: prompts/{prompt_file}")

    # Execution
    success = True
    final_path = None

    if success: success = stage_a_write_content(state)
    if success: success = stage_b_structure_html(state)
    if success: success = stage_c_style_publish(state)

    if success and enable_images:
        if stage_d_brainstorm_images(state):
            if state.image_plans:
                stage_d_generate_images(state)
                stage_d_inject_images(state)
    
    if success:
        final_path = save_final_output(state)
        state.log(f"\n‚úÖ Final output: {final_path}")

    state.save_log()

    if success:
        print(f"\n‚úÖ PIPELINE COMPLETE! Check directory: {output_dir}")
        if final_path:
            print(f"üìÑ Open file: {final_path}")
    else:
        print("\n‚ùå PIPELINE FAILED. Check log.")

    return 0 if success else 1


if __name__ == "__main__":
    raise SystemExit(main())
